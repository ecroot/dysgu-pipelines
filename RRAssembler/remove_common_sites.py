"""
Some contigs appear to map vector inserts for example the LIG3comp parental line has a contig which maps to the
LIG3 locus, showing the mitochondrial LIG3 or nuclear complemented vector.
Also remove any contigs which map the DnHTER at chr5p

chr5:1251850-1295754
chr17:33306147-33332286

Other contigs may also represent common SVs which are not unique to the post-crisis clones.
These need to be filtered out.
This script is run straight after the analyse_contigs.py script
Next run break_contigs_into_Svs
"""

import pandas as pd
import networkx as nx
import sys
import analyse_contigs  # Use load contigs function
import map_reads_to_contigs  # Use the black list which has already been generated by mapping reads to conitgs
import glob
import numpy as np


def load_data(name_column="contig_names"):
    # input_file = "All_mapped_contigs.all.csv"
    input_file = "Best_alignments.all.csv"
    df = pd.read_csv(input_file)

    G = nx.Graph()  # Make a DiGraph for each sample, central nodes are the contigs, peripheral nodes point to
    # are the break points. That way multiple graphs can be merged to find common breakpoints across samples and contigs

    # Make a set of breakpoints for each contig, then compare these to each other
    contig_bps = {}  # sample: contig: set of breakpoints

    bad_contigs = set([])

    print("Bad conitgs from map_reads_to_contigs.py", bad_contigs)


    #bad_intervals = [("chr5", 1251850, 1295754), ("chr17", 33306147, 33332286)]
    for contig, data in df.groupby(name_column):

        samp = data["sample"].iloc[0]
        if samp not in contig_bps:
            contig_bps[samp] = {}
        breakpoints = []
        #breakpoints = data[["tName", "tStart", "tName", "tEnd"]].values.tolist()
        for i, r in data.iterrows():
            breakpoints.append((r["tName"], r["tStart"]))
            breakpoints.append((r["tName"], r["tEnd"]))

        # Check any breakpoints in 5p or chr17 LIG3
        # hit = False
        # for b_chrom, b_pos in breakpoints:
        #     for chrom, start, end in bad_intervals:
        #         if b_chrom == chrom:
        #             if start <= b_pos <= end:
        #                 hit = True
        # if hit:
        #     bad_contigs.add(data[name_column].iloc[0])
        #     continue

        contig_bps[samp][contig] = set(breakpoints)

        central_node = "{}>{}".format(samp, contig)
        G.add_node(central_node, color="black")
        for bp in breakpoints:
            G.add_node(bp, color="grey")
            G.add_edge(bp, central_node)

    print("Number of conitgs mapping 5p or chr17 LIG3 locus", len(bad_contigs))


    sub_g = [G.subgraph(c) for c in nx.connected_components(G)]
    for gr in sub_g:
        data = gr.nodes(data=True)

        central = [j for j in data if j[1]["color"] == "black"]
        peripheral = [(j[0], gr.degree(j[0])) for j in data if j[1]["color"] == "grey"]

        if len(central) > 1:
            names = [j[0].split(">") for j in central]
            # Count the number of samples
            counts = set(i[0] for i in names)

            # Keep contigs which are from the same sample, even if they have a shared breakpoint
            if len(counts) > 1:
                [bad_contigs.add(i[1]) for i in names]

    print("Discarding", len(bad_contigs), "contigs")


    # Filter out bad contigs
    df = df[~df[name_column].isin(bad_contigs)]
    print(len(set(df[name_column])), "contigs kept")


    input_file = glob.glob("./regions_all/DB*/contigs.filtered.fasta")  # All
    input_file = [i for i in input_file if i.split("/")[-2]]

    #
    contigs = analyse_contigs.load_contigs(input_file, 150)

    allowed_names = set(df[name_column])
    contigs = {k: v for k, v in contigs.items() if k in allowed_names}

    print("N contigs", len(contigs.keys()))

    lengths = []
    max_l = 0
    cont_name = ""
    for k, v in contigs.items():
        lengths.append(len(v))
        if len(v) > max_l:
            max_l = len(v)
            cont_name = k

    print("Longest contig:", cont_name, max_l)

    a = np.array(sorted(lengths))
    sa = a.sum() / 2
    cum_sum = a.cumsum()
    for index, i in enumerate(cum_sum):
        if i > sa:
            break
    # print "N50: ", a[index]
    print("Mean length", np.mean(a))

    df.to_csv("All_mapped_contigs.unique.csv")

    ### check around interval tree
    return


