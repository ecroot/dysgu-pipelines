"""
Need to check the quality of the contigs generated by mapping the reads to the contigs.
Remove contigs with very high coverage
Run this before analyse_contigs

Removes contigs with high coverage and which have too much sequence labelled as repetitive by tantan

"""

from skbio.alignment import StripedSmithWaterman
import pandas as pd
import glob
from collections import defaultdict
import numpy as np
import sys
from multiprocessing import Pool
import pickle
import networkx as nx
from ast import literal_eval
from subprocess import Popen, PIPE, call
import sys
import pysam


def load_contigs(p, target=False):
    conts = glob.glob(p)
    contigs = defaultdict(list)
    for path in conts:
        sample = path.split("/")[-2]
        contigs[sample] = defaultdict(str)
        lines = [i.strip() for i in open(path, "r").readlines()]
        name = None
        for l in lines:
            if l[0] == ">":
                name = l[1:]
            else:
                if target:
                    if name == target:
                        contigs[sample][name] += l
                # elif name in allowed_contigs:
                else:
                    contigs[sample][name] += l
    
    return {k: v for k, v in contigs.items() if len(v) > 0}


def load_reads(regions_dir):
    fastq = glob.glob(f"{regions_dir}/*/*.fastq")  # Pared and unpaired
    single_reads = {v.split("/")[1]: v for v in glob.glob(f"{regions_dir}/*/*singles.fastq.keep.fastq")}  # Path to the fastq files
    paired_reads = {v.split("/")[1]: v for v in glob.glob(f"{regions_dir}/*/*paired.fastq")}
    return single_reads, paired_reads


def worker(args):
    seq = args[0]
    reads = args[1]
    contig_name = args[2]
    rev = {"A": "T", "C": "G", "G": "C", "T": "A", "N": "N", "a": "t", "c": "g", "g": "c", "t": "a", "n": "n"}
    cov = np.zeros(len(seq))  # Add to coverage to see how much is covered
    query = StripedSmithWaterman(seq,
                                 gap_open_penalty=6,
                                 gap_extend_penalty=4,
                                 mismatch_score=-4, #-3,
                                 match_score=1) #2)

    # Make a graph with indexes as nodes. Use to find if the contig coverage has any gaps.
    # G = nx.Graph()
    for r in reads:
        aln_f = query(r)
        print(aln_f)
        aln_r = query("".join(rev[i] for i in r[::-1]))
        if aln_f.optimal_alignment_score >= aln_r.optimal_alignment_score:
            aln = aln_f
        else:
            aln = aln_r

        if aln.optimal_alignment_score < 140:
            continue

        cov[aln.query_begin: aln.query_end + 1] += 1
        # G.add_path(range(aln.query_begin, aln.query_end + 1))

    # e = sorted(G.edges())
    # print [(e[i], e[i+1]) for i in range(len(e)-1) if e[i][1] != e[i+1][0]]

    ub = len(np.where(cov == 0)[0])
    #
    cov_arr = cov.astype(int)
    mean_cov = np.mean(cov)
    # print(seq)
    # print("".join([i for i, j in zip(seq, cov_arr) if j > 0]))
    # print(list(cov_arr))
    # print(np.median(cov_arr))
    return [contig_name, ub, mean_cov, cov_arr]


def map_reads_to_contigs(args):
    reads, contigs, sample = args
    print(sample, len(reads))
    res = []

    for contig, seq in contigs.items():
        res.append(worker((seq, reads, contig)) + [sample])

    return res


def perform_mapping(reads, contigs):
    arg_map = []
    count = 0
    for k in contigs:  # For each sample
        r = reads[k]
        c = contigs[k]
        arg_map.append((r, c, k))
        # if count > 2:
        #     break
        # count += 1

    #pool = Pool(processes=8)
    #res = pool.map(map_reads_to_contigs, arg_map)
    res = map(map_reads_to_contigs, arg_map)
    pickle.dump(res, open("out_data/reads_to_contigs_res.pkl", "wb"))
    records = []
    for item in res:
        for r in item:
            records.append({"contig": r[0],
                            "unmapped_bases": r[1],
                            "mean_cov": r[2],
                            "sample": r[4],
                            "cov_arr": ",".join(map(str, r[3]))})

    pd.DataFrame.from_records(records).to_csv("out_data/reads_mapped_to_contigs.csv")


def analyse_bwa(conts, mapped, max_cov=45):
    # Make a coverage array for each contig
    cov_arrs = {k: np.zeros(len(v)) for k, v in conts.items()}
    # Find start and end coords from mapping results
    l1 = len(mapped)
    # Remove unmapped
    mapped = [i for i in mapped if not int(i[1]) & 4]
    print("Unmapped reads", l1 - len(mapped), "/", l1)
    for r in mapped:
        cig = r[5]
        ints = map(int, "".join([i if i.isdigit() else " " for i in cig]).split(" ")[:-1])
        codes = [i for i in cig if not i.isdigit()]
        alined_d = sum([i for i, j in zip(ints, codes) if j != "S" and j != "I"])
        mapping_end = int(r[3]) + alined_d
        if mapping_end > 125:
            cov_arrs[r[2]][int(r[3])-1:mapping_end-1] += 1

    # Keep the new mean coverage value for saving later
    keep = {}
    for k, v in cov_arrs.items():
        m = v.astype(float).mean()
        if m < max_cov:
            keep[k] = m
    
    return keep


def percent_repetitive(seq):
    with open("temp/for_tantan.fasta", "w") as o:
        o.write(">t\n{}\n".format(seq))
    call("tantan -x N temp/for_tantan.fasta > temp/tantan.res", shell=True)
    lines = "".join([i.strip() for i in open("temp/tantan.res", "r") if i[0] != ">"])
    return float(lines.count("N")) / len(lines)


def bwa_map(contigs, singles, pairs):

    # For each sample map reads to contigs using bwa
    for samp in contigs.keys():

        print(samp)
        # if samp != "DB53":
        #     continue

        # Create temp refernce for bwa
        with open("temp/contig.bwa_ref.fasta", "w") as out:
            for k, v in contigs[samp].items():
                out.write(">{}\n{}\n".format(k, v))

        # Map singles and pairs, use hard clipping
        call("bwa index temp/contig.bwa_ref.fasta", shell=True)
        call("bwa mem temp/contig.bwa_ref.fasta {} > temp/singles.sam".format(singles[samp]), shell=True)
        call("bwa mem -p temp/contig.bwa_ref.fasta {} > temp/pairs.sam".format(pairs[samp]), shell=True)

        mapped = [i.split("\t") for i in open("temp/singles.sam", "r") if i[0] != "@"] + \
                 [i.split("\t") for i in open("temp/pairs.sam", "r") if i[0] != "@"]

        keep = analyse_bwa(contigs[samp], mapped)
        too_rep = 0
        count = 0
        with open("regions_all/{}/contigs.filtered.fasta".format(samp), "w") as out:
            # Save a new copy of the contigs with proper names filled out
            for k, cov in keep.items():
                seq = contigs[samp][k]
                pct = percent_repetitive(seq)

                if pct < 0.8:  # 80%
                    new_name = "{samp}_length_{l}_mean_cov_{c}".format(samp=samp, l=len(seq), c=cov)
                    out.write(">{}\n{}\n".format(new_name, seq))
                else:
                    too_rep += 1
                count += 1

        print("Discarded repetitive", too_rep, "out of", count)


def get_dysgu_contigs(contigs, vcfs):
    vcfs = glob.glob(vcfs)
    conts = defaultdict(list)  # Sample: contigs
    for item in vcfs:
        samp = item.split("/")[-1].split(".")[0]
        f = pysam.VariantFile(item)
        for v in f.fetch():
            for c in ["CONTIGA", "CONTIGB"]:
                if c in set(dict(v.info).keys()):
                    conts[samp].append(v.info[c])
    
    # Much the same as bwa_map function
    for samp in contigs.keys():
        # Create the 'reads' which will be mapped to reference
        with open("temp/dysgu.fasta", "w") as out:
            for count, seq in enumerate(conts[samp]):
                out.write(">{}\n{}\n".format(count, seq))
        # Create temp refernce for bwa
        with open("temp/contig.bwa_ref.fasta", "w") as out:
            for k, v in contigs[samp].items():
                out.write(">{}\n{}\n".format(k, v))

        # Map singles and pairs, use hard clipping
        call("bwa index temp/contig.bwa_ref.fasta", shell=True)
        call("bwa mem temp/contig.bwa_ref.fasta temp/dysgu.fasta > temp/dysgu.sam", shell=True)

        mapped = [i.split("\t") for i in open("temp/dysgu.sam", "r") if i[0] != "@"]

        keep = analyse_bwa(contigs[samp], mapped)
        covered_contigs = {k: v for k, v in keep.items() if v > 0}

        with open("regions_all/{}/contigs.filtered.dysgu_mapped.fasta".format(samp), "w") as out:
            # Save a new copy of the contigs with proper names filled out
            for k, cov in covered_contigs.items():
                seq = contigs[samp][k]
                new_name = "{samp}_length_{l}_mean_cov_{c}".format(samp=samp, l=len(seq), c=cov)
                out.write(">{}\n{}\n".format(new_name, seq))
    return conts




#if __name__ == "__main__":
def main(regions_dir, vcfs):
    target = False  #"NODE_9_length_1501_cov_3.98201"
    contigs = load_contigs(f"{regions_dir}/*/contigs.fasta")

    # print contigs["DB101_length_1314_mean_cov_14.101978691"]

    # sys.exit()
    singles, pairs = load_reads(regions_dir)
    bwa_map(contigs, singles, pairs)

    dysgu_conts = get_dysgu_contigs(contigs, vcfs)
    reads = dysgu_conts
    perform_mapping(reads, contigs)
    # black_list = analyse_result(contigs)
    # print(black_list)
